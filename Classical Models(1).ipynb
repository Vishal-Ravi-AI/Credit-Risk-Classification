{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "19095032-ac9a-4107-8d84-a6d21a2ecbd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length: 1000\n",
      "              id product underwriter broker_id company_id  agent_id  \\\n",
      "0  n4c52a64d0464    boat        BHSI       NaN        NaN       NaN   \n",
      "1  c3de6b17abe0d    boat        BHSI       NaN        NaN       NaN   \n",
      "2  n3575ef11c846    boat        BHSI       NaN        NaN       NaN   \n",
      "3  n99c684a6d889    boat        BHSI       NaN        NaN       NaN   \n",
      "4  h25703bafabdf    boat        BHSI       NaN        NaN       NaN   \n",
      "\n",
      "  policy_number  cover_version invoice_number transaction_type  ...  \\\n",
      "0           NaN              1            NaN              new  ...   \n",
      "1           NaN              1            NaN              new  ...   \n",
      "2           NaN              1            NaN              new  ...   \n",
      "3           NaN              1            NaN              new  ...   \n",
      "4           NaN              1            NaN              new  ...   \n",
      "\n",
      "  Unnamed: 268 Unnamed: 269  Unnamed: 270 Unnamed: 271  Unnamed: 272  \\\n",
      "0          NaN          NaN           NaN          NaN           NaN   \n",
      "1          NaN          NaN           NaN          NaN           NaN   \n",
      "2          NaN          NaN           NaN          NaN           NaN   \n",
      "3          NaN          NaN           NaN          NaN           NaN   \n",
      "4          NaN          NaN           NaN          NaN           NaN   \n",
      "\n",
      "   Unnamed: 273  Unnamed: 274  Unnamed: 275  Unnamed: 276  Unnamed: 277  \n",
      "0           NaN           NaN           NaN           NaN           NaN  \n",
      "1           NaN           NaN           NaN           NaN           NaN  \n",
      "2           NaN           NaN           NaN           NaN           NaN  \n",
      "3           NaN           NaN           NaN           NaN           NaN  \n",
      "4           NaN           NaN           NaN           NaN           NaN  \n",
      "\n",
      "[5 rows x 278 columns]\n",
      "id               object\n",
      "product          object\n",
      "underwriter      object\n",
      "broker_id        object\n",
      "company_id       object\n",
      "                 ...   \n",
      "Unnamed: 273    float64\n",
      "Unnamed: 274    float64\n",
      "Unnamed: 275    float64\n",
      "Unnamed: 276    float64\n",
      "Unnamed: 277    float64\n",
      "Length: 278, dtype: object\n",
      "(1000, 278)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset and treat \"NULL\" as missing\n",
    "dataset = pd.read_csv(\"Cargo.csv\", na_values=[\"NULL\"])\n",
    "\n",
    "# Print the length of the dataset\n",
    "print(\"Dataset length:\", len(dataset))\n",
    "\n",
    "# Show the first few rows of the dataset\n",
    "print(dataset.head())\n",
    "\n",
    "# Display data types of the columns\n",
    "print(dataset.dtypes)\n",
    "\n",
    "# Display the shape of the dataset\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fdb56652-7093-4855-b72d-70a45dcf3f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset shape: (1000, 25)\n"
     ]
    }
   ],
   "source": [
    "# Drop all 'Unnamed:' columns\n",
    "dataset = dataset.loc[:, ~dataset.columns.str.startswith('Unnamed')]\n",
    "\n",
    "# Drop columns with more than 70% missing data\n",
    "missing_threshold = 0.70\n",
    "mostly_missing_cols = dataset.columns[dataset.isnull().mean() > missing_threshold].tolist()\n",
    "dataset.drop(mostly_missing_cols, axis=1, inplace=True)\n",
    "\n",
    "# Drop specific named columns if they exist\n",
    "named_columns_to_drop = [\n",
    "    'broker_id', 'company_id', 'agent_id', 'status', 'fsl', 'bound_timestamp', 'cbs',\n",
    "    'product', 'underwriter', 'cover_version', 'agent_comm', 'agent_gst', 'rater',\n",
    "    'old', 'wording', 'ahm_comm', 'refer_reason', 'purchase_price', 'discount_code',\n",
    "    'id', 'group_id', 'cid', 'quote_id', 'calculation', 'ip', 'url',\n",
    "    'transaction_type', 'direct', 'allow_renew', 'accounting', 'funding', 'premium', 'premium_gst', 'annual_premium_gst', 'fee',\n",
    "    'comm', 'comm_gst', 'comm_premium', 'discount_gst', 'renew_sent',\n",
    "    'fsl_ratio', 'gst_ratio', 'stamp_ratio', 'referral_timestamp', 'liability'\n",
    "]\n",
    "\n",
    "columns_present = [col for col in named_columns_to_drop if col in dataset.columns]\n",
    "dataset.drop(columns=columns_present, axis=1, inplace=True)\n",
    "\n",
    "dataset.to_excel(\"Cargo_cleaned_final.xlsx\", index=False)\n",
    "\n",
    "dataset = pd.read_excel(\"Cargo_cleaned_final.xlsx\")\n",
    "\n",
    "# Confirm cleanup\n",
    "print(\"Cleaned dataset shape:\", dataset.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7a3e8ddb-2b97-4fd9-86d8-8275454fc12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Final cleaned dataset shape: (767, 25)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load the Excel file\n",
    "dataset = pd.read_excel(\"Cargo_cleaned_final.xlsx\")\n",
    "\n",
    "# Step 2: Drop malformed 'category' rows\n",
    "malformed_value = 'LegalName:{\"EffectiveFrom\":null'\n",
    "dataset = dataset[dataset['category'] != malformed_value]\n",
    "\n",
    "# Step 3: Drop rows with missing values\n",
    "dataset.dropna(inplace=True)\n",
    "\n",
    "# Step 4: Drop duplicate rows\n",
    "dataset.drop_duplicates(inplace=True)\n",
    "\n",
    "# Step 5: Save the cleaned dataset\n",
    "dataset.to_excel(\"Cargo_cleaned_final_cleaned.xlsx\", index=False)\n",
    "\n",
    "# Step 6: Confirm shape\n",
    "print(\"✅ Final cleaned dataset shape:\", dataset.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "60084f8a-5904-4e03-a577-ee272481374d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset shape: (767, 53)\n"
     ]
    }
   ],
   "source": [
    "# ✅ Use this instead:\n",
    "dataset = pd.read_excel(\"Cargo_cleaned_final_cleaned.xlsx\")\n",
    "\n",
    "# Step 2: Replace 'dec' with 'no' in 'allow' (declined = bad credit)\n",
    "dataset['allow'] = dataset['allow'].replace('dec', 'no')\n",
    "\n",
    "# Step 3: Binary mapping\n",
    "binary_map = {'yes': 1, 'no': 0}\n",
    "dataset['allow'] = dataset['allow'].map(binary_map)\n",
    "dataset['pro_built'] = dataset['pro_built'].map(binary_map)\n",
    "dataset['water_skiers'] = dataset['water_skiers'].map(binary_map)\n",
    "\n",
    "# Step 4: Handle quote_timestamp (extract features)\n",
    "if 'quote_timestamp' in dataset.columns:\n",
    "    dataset['quote_timestamp'] = pd.to_datetime(dataset['quote_timestamp'], errors='coerce')\n",
    "    dataset['quote_hour'] = dataset['quote_timestamp'].dt.hour\n",
    "    dataset['quote_dayofweek'] = dataset['quote_timestamp'].dt.dayofweek\n",
    "    dataset['quote_month'] = dataset['quote_timestamp'].dt.month\n",
    "    dataset.drop('quote_timestamp', axis=1, inplace=True)\n",
    "\n",
    "# Step 5: Convert power_speed to categories\n",
    "if 'power_speed' in dataset.columns:\n",
    "    dataset['power_speed'] = dataset['power_speed'].replace('RecordLastUpdatedDate:null}', pd.NA)\n",
    "    speed_bins = {\n",
    "        'Up to 10 knots': 'Low',\n",
    "        'Up to 20 knots': 'Low',\n",
    "        'Up to 30 knots': 'Medium',\n",
    "        'Up to 40 knots': 'Medium',\n",
    "        'Up to 50 knots': 'High',\n",
    "        'Up to 61 knots': 'High'\n",
    "    }\n",
    "    dataset['power_speed_category'] = dataset['power_speed'].map(speed_bins)\n",
    "    dataset.drop('power_speed', axis=1, inplace=True)\n",
    "    dataset = pd.get_dummies(dataset, columns=['power_speed_category'], drop_first=True)\n",
    "\n",
    "# Step 6: One-hot encode selected nominal columns\n",
    "desired_nominal_columns = [\n",
    "    'boat_type', 'hull_material', 'storage_method',\n",
    "    'storage_state', 'underwriter_id', 'cover_type'\n",
    "]\n",
    "nominal_columns_present = [col for col in desired_nominal_columns if col in dataset.columns]\n",
    "dataset = pd.get_dummies(dataset, columns=nominal_columns_present, drop_first=True)\n",
    "\n",
    "# Step 7: Convert any remaining boolean columns to 0/1\n",
    "bool_cols = dataset.select_dtypes(include='bool').columns\n",
    "dataset[bool_cols] = dataset[bool_cols].astype(int)\n",
    "\n",
    "# Step 8: Save the final version\n",
    "dataset.to_excel(\"Cargo_final_timestamp_speed_encoded.xlsx\", index=False)\n",
    "\n",
    "# Step 9: Confirm shape\n",
    "print(\"Final dataset shape:\", dataset.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "55e7b9f7-ae67-4d37-8889-9900be4b0bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_scaled shape: (580, 51)\n",
      "X_test_scaled shape:  (146, 51)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# (plus any other imports you already have: pandas as pd, etc.)\n",
    "\n",
    "# Step 1: Drop rows with any missing values (NaNs)\n",
    "dataset_clean = dataset.dropna()\n",
    "\n",
    "# Step 2: Ensure only numeric features are used\n",
    "X = dataset_clean.drop('allow', axis=1)\n",
    "y = dataset_clean['allow']\n",
    "\n",
    "# Filter only numeric columns\n",
    "X_numeric = X.select_dtypes(include=['number'])\n",
    "\n",
    "# Step 3: Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_numeric, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Step 4: Normalize\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled  = scaler.transform(X_test)\n",
    "\n",
    "# Step 5: Confirm shape\n",
    "print(\"X_train_scaled shape:\", X_train_scaled.shape)\n",
    "print(\"X_test_scaled shape: \", X_test_scaled.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "70f78a1c-eab8-45a5-bca2-188a77f3b0ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " SVM with kernel = 'linear'\n",
      " Accuracy: 0.7876712328767124\n",
      " Confusion Matrix:\n",
      " [[ 15  27]\n",
      " [  4 100]]\n",
      " Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.36      0.49        42\n",
      "           1       0.79      0.96      0.87       104\n",
      "\n",
      "    accuracy                           0.79       146\n",
      "   macro avg       0.79      0.66      0.68       146\n",
      "weighted avg       0.79      0.79      0.76       146\n",
      "\n",
      "\n",
      " SVM with kernel = 'rbf'\n",
      " Accuracy: 0.7671232876712328\n",
      " Confusion Matrix:\n",
      " [[ 12  30]\n",
      " [  4 100]]\n",
      " Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.29      0.41        42\n",
      "           1       0.77      0.96      0.85       104\n",
      "\n",
      "    accuracy                           0.77       146\n",
      "   macro avg       0.76      0.62      0.63       146\n",
      "weighted avg       0.76      0.77      0.73       146\n",
      "\n",
      "\n",
      " SVM with kernel = 'sigmoid'\n",
      " Accuracy: 0.7397260273972602\n",
      " Confusion Matrix:\n",
      " [[  8  34]\n",
      " [  4 100]]\n",
      " Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.19      0.30        42\n",
      "           1       0.75      0.96      0.84       104\n",
      "\n",
      "    accuracy                           0.74       146\n",
      "   macro avg       0.71      0.58      0.57       146\n",
      "weighted avg       0.72      0.74      0.68       146\n",
      "\n",
      "\n",
      " SVM with kernel = 'poly'\n",
      " Accuracy: 0.7465753424657534\n",
      " Confusion Matrix:\n",
      " [[11 31]\n",
      " [ 6 98]]\n",
      " Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.26      0.37        42\n",
      "           1       0.76      0.94      0.84       104\n",
      "\n",
      "    accuracy                           0.75       146\n",
      "   macro avg       0.70      0.60      0.61       146\n",
      "weighted avg       0.73      0.75      0.71       146\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "kernels = ['linear', 'rbf', 'sigmoid', 'poly']\n",
    "\n",
    "for kernel in kernels:\n",
    "    print(f\"\\n SVM with kernel = '{kernel}'\")\n",
    "    \n",
    "    # Initialize and train\n",
    "    model = SVC(kernel=kernel, gamma='scale', random_state=42)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Evaluate\n",
    "    print(\" Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\" Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "    print(\" Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b47f0f06-d5e7-4e98-a4ca-e881752c8b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best Parameters: {'C': 50, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      " Best CV Accuracy: 0.8327586206896551\n",
      " Test Accuracy: 0.863013698630137\n",
      "\n",
      " Confusion Matrix:\n",
      " [[31 11]\n",
      " [ 9 95]]\n",
      "\n",
      " Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.74      0.76        42\n",
      "           1       0.90      0.91      0.90       104\n",
      "\n",
      "    accuracy                           0.86       146\n",
      "   macro avg       0.84      0.83      0.83       146\n",
      "weighted avg       0.86      0.86      0.86       146\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Step 1: Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 50],\n",
    "    'gamma': ['scale', 0.01, 0.1, 1],\n",
    "    'kernel': ['rbf']\n",
    "}\n",
    "\n",
    "# Step 2: Set up the grid search\n",
    "grid = GridSearchCV(SVC(), param_grid, cv=10, scoring='accuracy')\n",
    "grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Step 3: Display the best parameters and cross-validation score\n",
    "print(\" Best Parameters:\", grid.best_params_)\n",
    "print(\" Best CV Accuracy:\", grid.best_score_)\n",
    "\n",
    "# Step 4: Evaluate the best model on the test set\n",
    "best_model = grid.best_estimator_\n",
    "y_pred = best_model.predict(X_test_scaled)\n",
    "\n",
    "print(\" Test Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\n Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\n Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "28bf050f-6d2a-4dc6-947e-e5704af11de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy: 0.8561643835616438\n",
      "\n",
      " Confusion Matrix:\n",
      " [[34  8]\n",
      " [13 91]]\n",
      "\n",
      " Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.81      0.76        42\n",
      "           1       0.92      0.88      0.90       104\n",
      "\n",
      "    accuracy                           0.86       146\n",
      "   macro avg       0.82      0.84      0.83       146\n",
      "weighted avg       0.86      0.86      0.86       146\n",
      "\n",
      "\n",
      " Total Misclassifications (FP + FN): 21\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Step 1: Train the Decision Tree model\n",
    "tree_model = DecisionTreeClassifier(random_state=42)\n",
    "tree_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Step 2: Predict on the test set\n",
    "y_pred_tree = tree_model.predict(X_test_scaled)\n",
    "\n",
    "# Step 3: Evaluate model\n",
    "print(\" Accuracy:\", accuracy_score(y_test, y_pred_tree))\n",
    "cm = confusion_matrix(y_test, y_pred_tree)\n",
    "print(\"\\n Confusion Matrix:\\n\", cm)\n",
    "print(\"\\n Classification Report:\\n\", classification_report(y_test, y_pred_tree))\n",
    "\n",
    "# Step 4: Misclassification count (FP + FN)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "misclassified = fp + fn\n",
    "print(f\"\\n Total Misclassifications (FP + FN): {misclassified}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9a207b0c-f77d-4b23-9f4c-195b8a1af5a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy: 0.910958904109589\n",
      "\n",
      " Confusion Matrix:\n",
      " [[34  8]\n",
      " [ 5 99]]\n",
      "\n",
      " Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.81      0.84        42\n",
      "           1       0.93      0.95      0.94       104\n",
      "\n",
      "    accuracy                           0.91       146\n",
      "   macro avg       0.90      0.88      0.89       146\n",
      "weighted avg       0.91      0.91      0.91       146\n",
      "\n",
      "\n",
      " Total Misclassifications (FP + FN): 13\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Step 1: Initialize and train the Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Step 2: Predict on the test set\n",
    "y_pred_rf = rf_model.predict(X_test_scaled)\n",
    "\n",
    "# Step 3: Evaluate performance\n",
    "print(\" Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
    "cm = confusion_matrix(y_test, y_pred_rf)\n",
    "print(\"\\n Confusion Matrix:\\n\", cm)\n",
    "print(\"\\n Classification Report:\\n\", classification_report(y_test, y_pred_rf))\n",
    "\n",
    "# Step 4: Count total misclassifications (FP + FN)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "misclassified = fp + fn\n",
    "print(f\"\\n Total Misclassifications (FP + FN): {misclassified}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6e1a2597-11ad-492c-a64f-3e73db33c3a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\vr130\\downloads\\anaconda\\lib\\site-packages (3.0.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\vr130\\downloads\\anaconda\\lib\\site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\vr130\\downloads\\anaconda\\lib\\site-packages (from xgboost) (1.13.1)\n",
      " Accuracy: 0.9178082191780822\n",
      "\n",
      " Confusion Matrix:\n",
      " [[35  7]\n",
      " [ 5 99]]\n",
      "\n",
      " Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.83      0.85        42\n",
      "           1       0.93      0.95      0.94       104\n",
      "\n",
      "    accuracy                           0.92       146\n",
      "   macro avg       0.90      0.89      0.90       146\n",
      "weighted avg       0.92      0.92      0.92       146\n",
      "\n",
      "\n",
      " Total Misclassifications (FP + FN): 12\n"
     ]
    }
   ],
   "source": [
    "# Ensure xgboost is installed\n",
    "!pip install xgboost\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Step 1: Initialize and train XGBoost model\n",
    "xgb_model = xgb.XGBClassifier(eval_metric='logloss', random_state=42)  # Removed deprecated use_label_encoder\n",
    "xgb_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Step 2: Predict\n",
    "y_pred_xgb = xgb_model.predict(X_test_scaled)\n",
    "\n",
    "# Step 3: Evaluate\n",
    "print(\" Accuracy:\", accuracy_score(y_test, y_pred_xgb))\n",
    "cm = confusion_matrix(y_test, y_pred_xgb)\n",
    "print(\"\\n Confusion Matrix:\\n\", cm)\n",
    "print(\"\\n Classification Report:\\n\", classification_report(y_test, y_pred_xgb))\n",
    "\n",
    "# Step 4: Misclassification count (FP + FN)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "misclassified = fp + fn\n",
    "print(f\"\\n Total Misclassifications (FP + FN): {misclassified}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3c1efc15-0055-498e-9ea3-b6ef587b661b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy: 0.7808219178082192\n",
      "\n",
      " Confusion Matrix:\n",
      " [[ 11  31]\n",
      " [  1 103]]\n",
      "\n",
      " Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.26      0.41        42\n",
      "           1       0.77      0.99      0.87       104\n",
      "\n",
      "    accuracy                           0.78       146\n",
      "   macro avg       0.84      0.63      0.64       146\n",
      "weighted avg       0.81      0.78      0.73       146\n",
      "\n",
      "\n",
      " Total Misclassifications (FP + FN): 32\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Step 1: Train the model\n",
    "nb_model = GaussianNB()\n",
    "nb_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Step 2: Predict\n",
    "y_pred_nb = nb_model.predict(X_test_scaled)\n",
    "\n",
    "# Step 3: Evaluate\n",
    "print(\" Accuracy:\", accuracy_score(y_test, y_pred_nb))\n",
    "cm = confusion_matrix(y_test, y_pred_nb)\n",
    "print(\"\\n Confusion Matrix:\\n\", cm)\n",
    "print(\"\\n Classification Report:\\n\", classification_report(y_test, y_pred_nb))\n",
    "\n",
    "# Step 4: Total misclassifications (FP + FN)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "misclassified = fp + fn\n",
    "print(f\"\\n Total Misclassifications (FP + FN): {misclassified}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c97804b0-3c27-49c5-8a89-6e182da1ce24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy: 0.7808219178082192\n",
      "\n",
      " Confusion Matrix:\n",
      " [[ 11  31]\n",
      " [  1 103]]\n",
      "\n",
      " Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.26      0.41        42\n",
      "           1       0.77      0.99      0.87       104\n",
      "\n",
      "    accuracy                           0.78       146\n",
      "   macro avg       0.84      0.63      0.64       146\n",
      "weighted avg       0.81      0.78      0.73       146\n",
      "\n",
      "\n",
      " Total Misclassifications (FP + FN): 32\n",
      "\n",
      " Custom Cost (FP×1 + FN×10): 41\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Step 1: Train the model\n",
    "nb_model = GaussianNB()\n",
    "nb_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Step 2: Predict\n",
    "y_pred_nb = nb_model.predict(X_test_scaled)\n",
    "\n",
    "# Step 3: Evaluate\n",
    "print(\" Accuracy:\", accuracy_score(y_test, y_pred_nb))\n",
    "cm = confusion_matrix(y_test, y_pred_nb)\n",
    "print(\"\\n Confusion Matrix:\\n\", cm)\n",
    "print(\"\\n Classification Report:\\n\", classification_report(y_test, y_pred_nb))\n",
    "\n",
    "# Step 4: Total misclassifications (FP + FN)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "misclassified = fp + fn\n",
    "print(f\"\\n Total Misclassifications (FP + FN): {misclassified}\")\n",
    "\n",
    "# Step 5: Custom cost matrix evaluation based on your business priority\n",
    "# Priority: Minimize FN (good credit wrongly rejected)\n",
    "cost_fp = 1   # Less severe\n",
    "cost_fn = 10  # Very costly\n",
    "total_custom_cost = (fp * cost_fp) + (fn * cost_fn)\n",
    "\n",
    "print(f\"\\n Custom Cost (FP×1 + FN×10): {total_custom_cost}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "051eb8d3-f376-4602-9d66-cba3175fa294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10-Fold Cross-Validation Accuracies: [0.724 0.776 0.793 0.724 0.793 0.655 0.741 0.793 0.81  0.672]\n",
      " Mean CV Accuracy: 0.748\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "# Define the model with default C and no class weighting\n",
    "log_reg = LogisticRegression(\n",
    "    max_iter=5000,\n",
    "    random_state=2,\n",
    "    solver='lbfgs'  # Optional but good to show explicitly\n",
    ")\n",
    "\n",
    "# Perform 10-fold cross-validation\n",
    "cv_scores = cross_val_score(log_reg, X_train_scaled, y_train, cv=10, scoring='accuracy')\n",
    "\n",
    "# Print results\n",
    "print(\" 10-Fold Cross-Validation Accuracies:\", np.round(cv_scores, 3))\n",
    "print(\" Mean CV Accuracy:\", np.round(cv_scores.mean(), 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5a5a50c4-119d-494b-bad7-4f1bd5262d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Test Set Accuracy: 0.747\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Step 1: Define and train the model\n",
    "log_reg = LogisticRegression(max_iter=5000, random_state=2, solver='lbfgs')\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Step 2: Predict on the test set\n",
    "y_pred = log_reg.predict(X_test_scaled)\n",
    "\n",
    "# Step 3: Evaluate accuracy\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\" Test Set Accuracy:\", round(test_accuracy, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4ebf3a5e-d6e8-4ed6-b48b-71f50d7bc6e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best Parameters: {'C': 10, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      " Best Cross-Validation Accuracy: 0.852\n",
      " Test Set Accuracy: 0.87\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'C': [1, 10],\n",
    "    'solver': ['liblinear', 'saga']\n",
    "}\n",
    "\n",
    "# Setup the model\n",
    "log_reg = LogisticRegression(max_iter=5000, random_state=2)\n",
    "\n",
    "# Grid Search with 10-fold cross-validation\n",
    "grid = GridSearchCV(log_reg, param_grid, cv=10, scoring='accuracy')\n",
    "grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Best parameters and CV score\n",
    "print(\" Best Parameters:\", grid.best_params_)\n",
    "print(\" Best Cross-Validation Accuracy:\", round(grid.best_score_, 3))\n",
    "\n",
    "# Test accuracy of best model\n",
    "best_model = grid.best_estimator_\n",
    "y_pred_test = best_model.predict(X_test_scaled)\n",
    "test_acc = accuracy_score(y_test, y_pred_test)\n",
    "print(\" Test Set Accuracy:\", round(test_acc, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3016ca79-f07a-457f-9669-f79b124b9f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6281 - loss: 0.6506 - val_accuracy: 0.6034 - val_loss: 0.6746\n",
      "Epoch 2/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7291 - loss: 0.5628 - val_accuracy: 0.6034 - val_loss: 0.6479\n",
      "Epoch 3/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7286 - loss: 0.5367 - val_accuracy: 0.6724 - val_loss: 0.6173\n",
      "Epoch 4/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7750 - loss: 0.4846 - val_accuracy: 0.6724 - val_loss: 0.5842\n",
      "Epoch 5/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7669 - loss: 0.4735 - val_accuracy: 0.6552 - val_loss: 0.5925\n",
      "Epoch 6/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7770 - loss: 0.4592 - val_accuracy: 0.6552 - val_loss: 0.5739\n",
      "Epoch 7/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7747 - loss: 0.4631 - val_accuracy: 0.6379 - val_loss: 0.5995\n",
      "Epoch 8/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7673 - loss: 0.4559 - val_accuracy: 0.6379 - val_loss: 0.6166\n",
      "Epoch 9/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7662 - loss: 0.4520 - val_accuracy: 0.6379 - val_loss: 0.6234\n",
      "Epoch 10/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7836 - loss: 0.4188 - val_accuracy: 0.6724 - val_loss: 0.5726\n",
      "Epoch 11/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7897 - loss: 0.4056 - val_accuracy: 0.6379 - val_loss: 0.5413\n",
      "Epoch 12/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7827 - loss: 0.4119 - val_accuracy: 0.6897 - val_loss: 0.5743\n",
      "Epoch 13/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7875 - loss: 0.4133 - val_accuracy: 0.7069 - val_loss: 0.5601\n",
      "Epoch 14/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8041 - loss: 0.4045 - val_accuracy: 0.6897 - val_loss: 0.5717\n",
      "Epoch 15/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8188 - loss: 0.3811 - val_accuracy: 0.7241 - val_loss: 0.5890\n",
      "Epoch 16/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7889 - loss: 0.3876 - val_accuracy: 0.6897 - val_loss: 0.5930\n",
      "Epoch 17/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8047 - loss: 0.3636 - val_accuracy: 0.6724 - val_loss: 0.6104\n",
      "Epoch 18/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8060 - loss: 0.3781 - val_accuracy: 0.6724 - val_loss: 0.6396\n",
      "Epoch 19/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8338 - loss: 0.3460 - val_accuracy: 0.7069 - val_loss: 0.6207\n",
      "Epoch 20/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8201 - loss: 0.3442 - val_accuracy: 0.6724 - val_loss: 0.6350\n",
      "Epoch 21/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8330 - loss: 0.3302 - val_accuracy: 0.6724 - val_loss: 0.7103\n",
      "Epoch 22/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8226 - loss: 0.3553 - val_accuracy: 0.6724 - val_loss: 0.7226\n",
      "Epoch 23/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8332 - loss: 0.3404 - val_accuracy: 0.6724 - val_loss: 0.6899\n",
      "Epoch 24/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8468 - loss: 0.3129 - val_accuracy: 0.6724 - val_loss: 0.7101\n",
      "Epoch 25/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8315 - loss: 0.3267 - val_accuracy: 0.6724 - val_loss: 0.7896\n",
      "Epoch 26/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8292 - loss: 0.3532 - val_accuracy: 0.6724 - val_loss: 0.7729\n",
      "Epoch 27/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8296 - loss: 0.3254 - val_accuracy: 0.6897 - val_loss: 0.7322\n",
      "Epoch 28/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8413 - loss: 0.3266 - val_accuracy: 0.6897 - val_loss: 0.7044\n",
      "Epoch 29/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8521 - loss: 0.2851 - val_accuracy: 0.6897 - val_loss: 0.7046\n",
      "Epoch 30/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8784 - loss: 0.2991 - val_accuracy: 0.6897 - val_loss: 0.7199\n",
      "Epoch 31/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8472 - loss: 0.2958 - val_accuracy: 0.7069 - val_loss: 0.7571\n",
      "Epoch 32/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8382 - loss: 0.3505 - val_accuracy: 0.6724 - val_loss: 0.8041\n",
      "Epoch 33/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8716 - loss: 0.2911 - val_accuracy: 0.6724 - val_loss: 0.8593\n",
      "Epoch 34/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8819 - loss: 0.2555 - val_accuracy: 0.6724 - val_loss: 0.7686\n",
      "Epoch 35/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8204 - loss: 0.3389 - val_accuracy: 0.7069 - val_loss: 0.8099\n",
      "Epoch 36/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8779 - loss: 0.2835 - val_accuracy: 0.6897 - val_loss: 0.7902\n",
      "Epoch 37/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8526 - loss: 0.2824 - val_accuracy: 0.6724 - val_loss: 0.8201\n",
      "Epoch 38/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9025 - loss: 0.2566 - val_accuracy: 0.6897 - val_loss: 0.8484\n",
      "Epoch 39/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8834 - loss: 0.2475 - val_accuracy: 0.6897 - val_loss: 0.7922\n",
      "Epoch 40/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8628 - loss: 0.2900 - val_accuracy: 0.7069 - val_loss: 0.8475\n",
      "Epoch 41/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8610 - loss: 0.2637 - val_accuracy: 0.6897 - val_loss: 0.9456\n",
      "Epoch 42/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8892 - loss: 0.2697 - val_accuracy: 0.6897 - val_loss: 0.9416\n",
      "Epoch 43/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8702 - loss: 0.2582 - val_accuracy: 0.6724 - val_loss: 0.9378\n",
      "Epoch 44/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8628 - loss: 0.2895 - val_accuracy: 0.7069 - val_loss: 0.8600\n",
      "Epoch 45/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8941 - loss: 0.2633 - val_accuracy: 0.7069 - val_loss: 0.8837\n",
      "Epoch 46/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8908 - loss: 0.2477 - val_accuracy: 0.6897 - val_loss: 0.9393\n",
      "Epoch 47/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8849 - loss: 0.2567 - val_accuracy: 0.6724 - val_loss: 0.9677\n",
      "Epoch 48/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8834 - loss: 0.2598 - val_accuracy: 0.6724 - val_loss: 1.0472\n",
      "Epoch 49/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8694 - loss: 0.2729 - val_accuracy: 0.6897 - val_loss: 0.9691\n",
      "Epoch 50/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9004 - loss: 0.2472 - val_accuracy: 0.7069 - val_loss: 0.8986\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 31ms/stepWARNING:tensorflow:5 out of the last 16 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x00000285A9D06480> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\n",
      " Test Accuracy: 0.815068493150685\n",
      "\n",
      " Confusion Matrix:\n",
      " [[23 19]\n",
      " [ 8 96]]\n",
      "\n",
      " Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.55      0.63        42\n",
      "           1       0.83      0.92      0.88       104\n",
      "\n",
      "    accuracy                           0.82       146\n",
      "   macro avg       0.79      0.74      0.75       146\n",
      "weighted avg       0.81      0.82      0.81       146\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# !pip install tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "#  Step 1: Define the model\n",
    "model = Sequential([\n",
    "    Input(shape=(X_train_scaled.shape[1],)),  # Clean input layer\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  # Binary classification\n",
    "])\n",
    "\n",
    "#  Step 2: Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "#  Step 3: Train the model\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=16,\n",
    "    validation_split=0.1,\n",
    "    verbose=1  # You can set this to 0 for silent training\n",
    ")\n",
    "\n",
    "#  Step 4: Predict on the test set\n",
    "y_pred_proba = model.predict(X_test_scaled)\n",
    "y_pred = (y_pred_proba >= 0.5).astype(int)\n",
    "\n",
    "#  Step 5: Evaluate the model\n",
    "print(\"\\n Test Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Confusion matrix and classification report\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\n Confusion Matrix:\\n\", cm)\n",
    "print(\"\\n Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6d31457d-5168-441f-9a47-6a8eb6fe0005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7092 - loss: 0.6267 - val_accuracy: 0.6034 - val_loss: 0.6843\n",
      "Epoch 2/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7084 - loss: 0.5677 - val_accuracy: 0.6207 - val_loss: 0.6476\n",
      "Epoch 3/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7554 - loss: 0.5107 - val_accuracy: 0.6897 - val_loss: 0.6016\n",
      "Epoch 4/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7477 - loss: 0.5117 - val_accuracy: 0.6897 - val_loss: 0.5919\n",
      "Epoch 5/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7933 - loss: 0.4446 - val_accuracy: 0.6724 - val_loss: 0.5790\n",
      "Epoch 6/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7970 - loss: 0.4162 - val_accuracy: 0.6379 - val_loss: 0.5517\n",
      "Epoch 7/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7993 - loss: 0.4385 - val_accuracy: 0.6724 - val_loss: 0.5599\n",
      "Epoch 8/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7705 - loss: 0.4373 - val_accuracy: 0.6724 - val_loss: 0.6140\n",
      "Epoch 9/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7630 - loss: 0.4511 - val_accuracy: 0.6552 - val_loss: 0.6280\n",
      "Epoch 10/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7926 - loss: 0.4191 - val_accuracy: 0.6724 - val_loss: 0.5497\n",
      "Epoch 11/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7889 - loss: 0.4132 - val_accuracy: 0.6724 - val_loss: 0.5370\n",
      "Epoch 12/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7948 - loss: 0.4177 - val_accuracy: 0.6897 - val_loss: 0.5402\n",
      "Epoch 13/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8236 - loss: 0.3819 - val_accuracy: 0.6724 - val_loss: 0.5609\n",
      "Epoch 14/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7996 - loss: 0.4141 - val_accuracy: 0.6552 - val_loss: 0.5882\n",
      "Epoch 15/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8513 - loss: 0.3296 - val_accuracy: 0.6552 - val_loss: 0.6658\n",
      "Epoch 16/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8348 - loss: 0.3493 - val_accuracy: 0.6897 - val_loss: 0.5904\n",
      "Epoch 16: early stopping\n",
      "Restoring model weights from the end of the best epoch: 11.\n",
      "WARNING:tensorflow:5 out of the last 11 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x00000285A9D95260> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\n",
      " Training Accuracy: 0.7896551724137931\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\n",
      " Test Accuracy: 0.7602739726027398\n",
      "\n",
      " Confusion Matrix:\n",
      " [[17 25]\n",
      " [10 94]]\n",
      "\n",
      " Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.40      0.49        42\n",
      "           1       0.79      0.90      0.84       104\n",
      "\n",
      "    accuracy                           0.76       146\n",
      "   macro avg       0.71      0.65      0.67       146\n",
      "weighted avg       0.74      0.76      0.74       146\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# !pip install tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "#  Step 1: Define the model\n",
    "model = Sequential([\n",
    "    Input(shape=(X_train_scaled.shape[1],)),  # Input layer\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  # Binary classification\n",
    "])\n",
    "\n",
    "#  Step 2: Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "#  Step 3: Add early stopping\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "#  Step 4: Train the model\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=16,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "#  Step 5a: Evaluate on training set\n",
    "y_train_pred_proba = model.predict(X_train_scaled)\n",
    "y_train_pred = (y_train_pred_proba >= 0.5).astype(int)\n",
    "print(\"\\n Training Accuracy:\", accuracy_score(y_train, y_train_pred))\n",
    "\n",
    "#  Step 5b: Evaluate on test set\n",
    "y_pred_proba = model.predict(X_test_scaled)\n",
    "y_pred = (y_pred_proba >= 0.5).astype(int)\n",
    "print(\"\\n Test Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "#  Step 6: Final evaluation metrics\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\n Confusion Matrix:\\n\", cm)\n",
    "print(\"\\n Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b465a3bc-2a0a-46e5-a82c-b8dc207ec1d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7434 - loss: 0.6158 - val_accuracy: 0.6034 - val_loss: 0.6355\n",
      "Epoch 2/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7219 - loss: 0.5297 - val_accuracy: 0.6034 - val_loss: 0.5976\n",
      "Epoch 3/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7341 - loss: 0.4889 - val_accuracy: 0.6552 - val_loss: 0.6363\n",
      "Epoch 4/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7883 - loss: 0.4630 - val_accuracy: 0.6724 - val_loss: 0.5895\n",
      "Epoch 5/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7856 - loss: 0.4216 - val_accuracy: 0.6897 - val_loss: 0.5931\n",
      "Epoch 6/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7979 - loss: 0.4111 - val_accuracy: 0.6897 - val_loss: 0.5808\n",
      "Epoch 7/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8020 - loss: 0.3963 - val_accuracy: 0.6897 - val_loss: 0.6064\n",
      "Epoch 8/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8074 - loss: 0.3825 - val_accuracy: 0.6724 - val_loss: 0.5351\n",
      "Epoch 9/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8229 - loss: 0.3676 - val_accuracy: 0.7414 - val_loss: 0.5300\n",
      "Epoch 10/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8314 - loss: 0.3545 - val_accuracy: 0.6897 - val_loss: 0.6355\n",
      "Epoch 11/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8542 - loss: 0.3167 - val_accuracy: 0.7069 - val_loss: 0.5846\n",
      "Epoch 12/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8537 - loss: 0.3125 - val_accuracy: 0.6724 - val_loss: 0.7530\n",
      "Epoch 13/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8501 - loss: 0.3305 - val_accuracy: 0.6897 - val_loss: 0.6303\n",
      "Epoch 14/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8148 - loss: 0.3276 - val_accuracy: 0.7069 - val_loss: 0.6958\n",
      "Epoch 15/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8481 - loss: 0.3335 - val_accuracy: 0.6552 - val_loss: 0.7744\n",
      "Epoch 16/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8408 - loss: 0.3102 - val_accuracy: 0.7414 - val_loss: 0.5789\n",
      "Epoch 17/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8530 - loss: 0.3156 - val_accuracy: 0.7069 - val_loss: 0.6988\n",
      "Epoch 18/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8832 - loss: 0.2494 - val_accuracy: 0.6552 - val_loss: 0.9144\n",
      "Epoch 19/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8597 - loss: 0.2817 - val_accuracy: 0.6724 - val_loss: 0.8046\n",
      "Epoch 20/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8349 - loss: 0.2967 - val_accuracy: 0.6724 - val_loss: 0.8868\n",
      "Epoch 21/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8602 - loss: 0.2666 - val_accuracy: 0.6897 - val_loss: 0.8330\n",
      "Epoch 22/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8645 - loss: 0.2817 - val_accuracy: 0.7069 - val_loss: 0.7968\n",
      "Epoch 23/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8738 - loss: 0.2399 - val_accuracy: 0.6897 - val_loss: 0.7547\n",
      "Epoch 24/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8651 - loss: 0.2734 - val_accuracy: 0.7586 - val_loss: 0.7939\n",
      "Epoch 25/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8925 - loss: 0.2496 - val_accuracy: 0.6552 - val_loss: 1.1385\n",
      "Epoch 26/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8541 - loss: 0.2813 - val_accuracy: 0.7414 - val_loss: 0.7183\n",
      "Epoch 27/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8847 - loss: 0.2454 - val_accuracy: 0.7069 - val_loss: 0.8796\n",
      "Epoch 28/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9052 - loss: 0.2135 - val_accuracy: 0.6552 - val_loss: 0.9521\n",
      "Epoch 29/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8747 - loss: 0.2344 - val_accuracy: 0.6897 - val_loss: 1.0183\n",
      "Epoch 30/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8750 - loss: 0.2457 - val_accuracy: 0.7241 - val_loss: 0.7515\n",
      "Epoch 31/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8623 - loss: 0.2406 - val_accuracy: 0.7069 - val_loss: 0.8296\n",
      "Epoch 32/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9143 - loss: 0.2215 - val_accuracy: 0.7069 - val_loss: 0.9335\n",
      "Epoch 33/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8947 - loss: 0.2090 - val_accuracy: 0.6897 - val_loss: 0.8270\n",
      "Epoch 34/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9017 - loss: 0.1995 - val_accuracy: 0.7069 - val_loss: 0.8745\n",
      "Epoch 35/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8698 - loss: 0.2557 - val_accuracy: 0.6724 - val_loss: 1.0680\n",
      "Epoch 36/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9075 - loss: 0.2158 - val_accuracy: 0.6897 - val_loss: 1.0043\n",
      "Epoch 37/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8740 - loss: 0.2291 - val_accuracy: 0.7241 - val_loss: 0.9800\n",
      "Epoch 38/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9251 - loss: 0.1906 - val_accuracy: 0.6897 - val_loss: 1.0903\n",
      "Epoch 39/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8811 - loss: 0.1968 - val_accuracy: 0.6552 - val_loss: 1.1001\n",
      "Epoch 40/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9031 - loss: 0.2039 - val_accuracy: 0.7241 - val_loss: 0.9835\n",
      "Epoch 41/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9288 - loss: 0.1713 - val_accuracy: 0.6552 - val_loss: 1.2544\n",
      "Epoch 42/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8621 - loss: 0.2754 - val_accuracy: 0.7414 - val_loss: 0.9532\n",
      "Epoch 43/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8984 - loss: 0.2253 - val_accuracy: 0.7241 - val_loss: 0.9157\n",
      "Epoch 44/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9480 - loss: 0.1921 - val_accuracy: 0.6897 - val_loss: 0.9873\n",
      "Epoch 45/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8749 - loss: 0.2711 - val_accuracy: 0.6724 - val_loss: 0.9737\n",
      "Epoch 46/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8790 - loss: 0.2335 - val_accuracy: 0.7414 - val_loss: 0.8906\n",
      "Epoch 47/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9163 - loss: 0.1824 - val_accuracy: 0.6897 - val_loss: 1.1023\n",
      "Epoch 48/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9324 - loss: 0.1431 - val_accuracy: 0.6897 - val_loss: 0.9367\n",
      "Epoch 49/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9318 - loss: 0.1754 - val_accuracy: 0.7241 - val_loss: 1.0201\n",
      "Epoch 50/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9396 - loss: 0.1651 - val_accuracy: 0.7241 - val_loss: 0.9724\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\n",
      " Test Accuracy: 0.8493150684931506\n",
      "\n",
      " Confusion Matrix:\n",
      " [[31 11]\n",
      " [11 93]]\n",
      "\n",
      " Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.74      0.74        42\n",
      "           1       0.89      0.89      0.89       104\n",
      "\n",
      "    accuracy                           0.85       146\n",
      "   macro avg       0.82      0.82      0.82       146\n",
      "weighted avg       0.85      0.85      0.85       146\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# !pip install tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Step 1: Define the deeper model\n",
    "model = Sequential([\n",
    "    Input(shape=(X_train_scaled.shape[1],)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Step 2: Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Step 3: Train the model\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=16,\n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Step 4: Predict on the test set\n",
    "y_pred_proba = model.predict(X_test_scaled)\n",
    "y_pred = (y_pred_proba >= 0.5).astype(int)\n",
    "\n",
    "# Step 5: Evaluate the model\n",
    "print(\"\\n Test Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\n Confusion Matrix:\\n\", cm)\n",
    "print(\"\\n Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "68c60298-5b26-4d83-a89c-9c8361ce52bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Pipeline trained and saved as xgb_credit_risk_pipeline_final.pkl\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "#  Load your final feature file\n",
    "df_final = pd.read_excel(\"Cargo_final_timestamp_speed_encoded.xlsx\")\n",
    "\n",
    "#  Split into X, y\n",
    "X_final = df_final.drop(\"allow\", axis=1)\n",
    "y_final = df_final[\"allow\"]\n",
    "\n",
    "#  Identify numeric vs. categorical\n",
    "numeric_cols     = X_final.select_dtypes(include=\"number\").columns.tolist()\n",
    "categorical_cols = X_final.select_dtypes(include=\"object\").columns.tolist()\n",
    "\n",
    "#  Build the ColumnTransformer\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", MinMaxScaler(),               numeric_cols),\n",
    "    (\"cat\", OneHotEncoder(drop=\"first\", sparse_output=False), categorical_cols),\n",
    "])\n",
    "\n",
    "#  Chain into a Pipeline\n",
    "pipeline_final = Pipeline([\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"clf\", XGBClassifier(eval_metric=\"logloss\", random_state=42)),\n",
    "])\n",
    "\n",
    "#  Fit on all data\n",
    "pipeline_final.fit(X_final, y_final)\n",
    "\n",
    "#  Save to disk\n",
    "joblib.dump(pipeline_final, \"xgb_credit_risk_pipeline_final.pkl\")\n",
    "print(\" Pipeline trained and saved as xgb_credit_risk_pipeline_final.pkl\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3cb5ee27-a91b-4642-9634-0396d944ca41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in cwd: ['.anaconda', '.conda', '.condarc', '.continuum', '.ipynb_checkpoints', '.ipython', '.jupyter', '.keras', '.matplotlib', 'app.py', 'AppData', 'Application Data', 'batch_score.py', 'Cargo.csv', 'Cargo.ipynb', 'Cargo_cleaned_final.xlsx', 'Cargo_cleaned_final_cleaned.xlsx', 'Cargo_encoded_final.xlsx', 'Cargo_encoded_nominal.xlsx', 'Cargo_final_timestamp_speed_encoded.xlsx', 'Contacts', 'Cookies', 'Desktop', 'Documents', 'Downloads', 'Favorites', 'Links', 'Local Settings', 'Microsoft', 'Music', 'My Documents', 'NetHood', 'new_apps.csv', 'NTUSER.DAT', 'ntuser.dat.LOG1', 'ntuser.dat.LOG2', 'NTUSER.DAT{157062e8-0d6a-11f0-8df8-f7a9bdb53740}.TM.blf', 'NTUSER.DAT{157062e8-0d6a-11f0-8df8-f7a9bdb53740}.TMContainer00000000000000000001.regtrans-ms', 'NTUSER.DAT{157062e8-0d6a-11f0-8df8-f7a9bdb53740}.TMContainer00000000000000000002.regtrans-ms', 'ntuser.ini', 'OneDrive', 'Pictures', 'PrintHood', 'Recent', 'run_batch_score.bat', 'Saved Games', 'scored_apps.csv', 'scored_full_dataset.xlsx', 'Searches', 'SendTo', 'Start Menu', 'Templates', 'Untitled.ipynb', 'untitled.py', 'Untitled1.ipynb', 'Videos', 'xgb_credit_risk_pipeline_final.pkl']\n",
      "Predicted class: 0   (1 = approved/good risk, 0 = declined/bad risk)\n",
      "Probabilities → declined(0): 0.805, approved(1): 0.195\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "# 0. (Optional) Verify files\n",
    "print(\"Files in cwd:\", os.listdir())\n",
    "\n",
    "# 1. Load your trained pipeline\n",
    "pipeline = joblib.load(\"xgb_credit_risk_pipeline_final.pkl\")\n",
    "\n",
    "# 2. Read the cleaned feature file\n",
    "df = pd.read_excel(\"Cargo_final_timestamp_speed_encoded.xlsx\")\n",
    "feature_cols = df.drop(\"allow\", axis=1).columns.tolist()\n",
    "\n",
    "# 3. Split features by dtype\n",
    "numeric_cols     = df[feature_cols].select_dtypes(include=['number']).columns.tolist()\n",
    "categorical_cols = df[feature_cols].select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# 4. Build dummy record\n",
    "dummy = {}\n",
    "for col in feature_cols:\n",
    "    if col in numeric_cols:\n",
    "        dummy[col] = 0                    \n",
    "    else:\n",
    "        dummy[col] = df[col].mode()[0]   \n",
    "\n",
    "# 5. Tweak a few numerics for a “sensible” scenario\n",
    "dummy.update({\n",
    "    \"calculated_premium\": 50_000,\n",
    "    \"quote_hour\":            9,\n",
    "    \"quote_dayofweek\":       0,\n",
    "    \"quote_month\":          12\n",
    "})\n",
    "\n",
    "X_dummy = pd.DataFrame([dummy], columns=feature_cols)\n",
    "\n",
    "# 6. Predict\n",
    "pred_class = pipeline.predict(X_dummy)[0]\n",
    "pred_proba = pipeline.predict_proba(X_dummy)[0]\n",
    "\n",
    "print(f\"Predicted class: {pred_class}   (1 = approved/good risk, 0 = declined/bad risk)\")\n",
    "print(f\"Probabilities → declined(0): {pred_proba[0]:.3f}, approved(1): {pred_proba[1]:.3f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "95a85055-8108-4f3d-8c80-8a158d9121ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy input → class=0  (1=approved, 0=declined),  P(approve)=0.027\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Load pipeline & feature list\n",
    "pipeline     = joblib.load(\"xgb_credit_risk_pipeline_final.pkl\")\n",
    "df           = pd.read_excel(\"Cargo_final_timestamp_speed_encoded.xlsx\")\n",
    "feature_cols = df.drop(\"allow\", axis=1).columns.tolist()\n",
    "\n",
    "# 2. Start from a “base” of zeros/modes so you don’t have to set 50+ fields manually\n",
    "num_cols = df[feature_cols].select_dtypes(include='number').columns\n",
    "base_dummy = {\n",
    "    c: (0 if c in num_cols else df[c].mode()[0])\n",
    "    for c in feature_cols\n",
    "}\n",
    "\n",
    "# 3. Now *override* with the exact dummy values you want to test\n",
    "dummy = base_dummy.copy()\n",
    "dummy.update({\n",
    "    # Numeric features\n",
    "    \"calculated_premium\": 2500,\n",
    "    \"quote_hour\":            14,\n",
    "    \"quote_dayofweek\":        1,   # Tuesday\n",
    "    \"quote_month\":            3,   # March\n",
    "    \"half_excess\":          500,   # example deductible\n",
    "    \"excess\":              1000,\n",
    "    # One-hot flags (set the relevant ones to 1)\n",
    "    \"boat_type_HBP\":           1,\n",
    "    \"storage_method_SRE\":      0,\n",
    "    # … you can flip any other one-hot here as needed …\n",
    "})\n",
    "\n",
    "# 4. Build DataFrame & predict\n",
    "X_dummy = pd.DataFrame([dummy], columns=feature_cols)\n",
    "pred     = pipeline.predict(X_dummy)[0]\n",
    "proba    = pipeline.predict_proba(X_dummy)[0][1]\n",
    "\n",
    "print(f\"Dummy input → class={pred}  (1=approved, 0=declined),  P(approve)={proba:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a6fdce81-6134-474f-9445-bc48c6f09633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features for row 30\n",
      "calculated_premium annual_premium stamp_duty fee_gst   total excess discount_total half_excess category storage_postcode purchase_date sum_insured year_built pro_built water_skiers previous_claims quote_hour quote_dayofweek quote_month power_speed_category_Low boat_type_CRU boat_type_HBP boat_type_LAU boat_type_PON boat_type_RUN boat_type_SKI hull_material_CBF hull_material_FIB hull_material_KEV hull_material_PLA hull_material_RUB hull_material_STE hull_material_TIM storage_method_COM storage_method_DRY storage_method_FAA storage_method_GAR storage_method_HAR storage_method_JET storage_method_MAR storage_method_PON storage_method_SRE storage_method_SWI storage_state_NSW storage_state_NT storage_state_QLD storage_state_SA storage_state_TAS storage_state_VIC storage_state_WA underwriter_id_t4b354839eb36 cover_type_Comprehensive\n",
      "             526.6         500.27      49.53    5.27  657.76   1500          50.03           3    power             2330          2021       50000       2011         1            0               0       23.0             5.0         1.0                        0             0             0             0             0             1             0                 0                 0                 0                 0                 0                 0                 0                  0                  0                  0                  0                  0                  0                  0                  0                  0                  0                 1                0                 0                0                 0                 0                0                            0                        1\n",
      "\n",
      "Model prediction for row 30:\n",
      "  → Predicted class: 1  (1=approved, 0=declined)\n",
      "  → P(approve): 0.999\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Load pipeline & data\n",
    "pipeline   = joblib.load(\"xgb_credit_risk_pipeline_final.pkl\")\n",
    "df         = pd.read_excel(\"Cargo_final_timestamp_speed_encoded.xlsx\")\n",
    "\n",
    "# 2. Pick a row by index \n",
    "idx = 30\n",
    "row = df.iloc[idx]\n",
    "\n",
    "# 3. Extract its features (drop the target)\n",
    "X_new = row.drop(\"allow\").to_frame().T\n",
    "\n",
    "# 4. Display the raw feature values\n",
    "print(\"Features for row\", idx)\n",
    "print(X_new.to_string(index=False))\n",
    "\n",
    "# 5. Predict\n",
    "pred_class = pipeline.predict(X_new)[0]\n",
    "pred_proba = pipeline.predict_proba(X_new)[0][1]\n",
    "\n",
    "print(f\"\\nModel prediction for row {idx}:\")\n",
    "print(f\"  → Predicted class: {pred_class}  (1=approved, 0=declined)\")\n",
    "print(f\"  → P(approve): {pred_proba:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "87e7fc28-8cd4-4741-8bfe-41028a3c8975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.8376623376623377\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.62      0.69        45\n",
      "           1       0.86      0.93      0.89       109\n",
      "\n",
      "    accuracy                           0.84       154\n",
      "   macro avg       0.82      0.77      0.79       154\n",
      "weighted avg       0.83      0.84      0.83       154\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# 1. Load and split your data\n",
    "df = pd.read_excel(\"Cargo_final_timestamp_speed_encoded.xlsx\")\n",
    "X  = df.drop(\"allow\", axis=1)\n",
    "y  = df[\"allow\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.20,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "# 2. Re-create your preprocessing + model pipeline\n",
    "numeric_cols = X.select_dtypes(include='number').columns.tolist()\n",
    "cat_cols     = [c for c in X.columns if c not in numeric_cols]\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", MinMaxScaler(), numeric_cols),\n",
    "    (\"cat\", OneHotEncoder(drop=\"first\", sparse_output=False), cat_cols),\n",
    "])\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"clf\", XGBClassifier(eval_metric=\"logloss\", random_state=42)),\n",
    "])\n",
    "\n",
    "# 3. Fit only on the TRAIN split\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# 4. Evaluate on the untouched TEST split\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(\"Test accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "82368619-9d9d-4cf0-8f1d-5d378ebfb5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    allow_actual  predicted_class  approve_probability\n",
      "0              1                1                0.997\n",
      "1              0                0                0.012\n",
      "2              0                0                0.004\n",
      "3              1                1                1.000\n",
      "4              1                1                0.978\n",
      "5              1                1                0.918\n",
      "6              1                1                0.998\n",
      "7              1                1                0.973\n",
      "8              1                1                0.981\n",
      "9              0                0                0.119\n",
      "10             0                0                0.119\n",
      "11             1                1                0.786\n",
      "12             1                1                0.991\n",
      "13             0                0                0.056\n",
      "14             1                1                0.994\n",
      "15             0                0                0.081\n",
      "16             1                1                0.900\n",
      "17             1                1                0.982\n",
      "18             1                1                0.980\n",
      "19             1                1                0.991\n",
      "20             1                1                0.985\n",
      "21             1                1                0.989\n",
      "22             0                0                0.024\n",
      "23             1                1                0.998\n",
      "24             1                1                0.998\n",
      "25             1                1                0.986\n",
      "26             1                1                0.986\n",
      "27             1                1                0.996\n",
      "28             0                0                0.013\n",
      "29             1                1                0.994\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load pipeline & data\n",
    "pipeline = joblib.load(\"xgb_credit_risk_pipeline_final.pkl\")\n",
    "df       = pd.read_excel(\"Cargo_final_timestamp_speed_encoded.xlsx\")\n",
    "\n",
    "# First 30 rows\n",
    "X_first30 = df.drop(\"allow\", axis=1).iloc[:30]\n",
    "y_first30 = df[\"allow\"].iloc[:30]\n",
    "\n",
    "# Predictions\n",
    "preds  = pipeline.predict(X_first30)\n",
    "probas = pipeline.predict_proba(X_first30)[:,1]\n",
    "\n",
    "# Compile results\n",
    "results = pd.DataFrame({\n",
    "    \"allow_actual\":        y_first30.values,\n",
    "    \"predicted_class\":     preds,\n",
    "    \"approve_probability\": np.round(probas, 3)\n",
    "}, index=y_first30.index)\n",
    "\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a2ffb9b8-5827-455e-855b-2c3d7b2cab72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 30 rows: actual vs predicted\n",
      "\n",
      "    allow_actual  predicted_class  approve_probability\n",
      "0              1                1                0.997\n",
      "1              0                0                0.012\n",
      "2              0                0                0.004\n",
      "3              1                1                1.000\n",
      "4              1                1                0.978\n",
      "5              1                1                0.918\n",
      "6              1                1                0.998\n",
      "7              1                1                0.973\n",
      "8              1                1                0.981\n",
      "9              0                0                0.119\n",
      "10             0                0                0.119\n",
      "11             1                1                0.786\n",
      "12             1                1                0.991\n",
      "13             0                0                0.056\n",
      "14             1                1                0.994\n",
      "15             0                0                0.081\n",
      "16             1                1                0.900\n",
      "17             1                1                0.982\n",
      "18             1                1                0.980\n",
      "19             1                1                0.991\n",
      "20             1                1                0.985\n",
      "21             1                1                0.989\n",
      "22             0                0                0.024\n",
      "23             1                1                0.998\n",
      "24             1                1                0.998\n",
      "25             1                1                0.986\n",
      "26             1                1                0.986\n",
      "27             1                1                0.996\n",
      "28             0                0                0.013\n",
      "29             1                1                0.994\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load your trained pipeline and the cleaned dataset\n",
    "pipeline = joblib.load(\"xgb_credit_risk_pipeline_final.pkl\")\n",
    "df       = pd.read_excel(\"Cargo_final_timestamp_speed_encoded.xlsx\")\n",
    "\n",
    "# 2. Extract features and true labels for rows 0–29\n",
    "X_first30 = df.drop(\"allow\", axis=1).iloc[:30]\n",
    "y_first30 = df[\"allow\"].iloc[:30]\n",
    "\n",
    "# 3. Predict\n",
    "pred_classes = pipeline.predict(X_first30)\n",
    "pred_probs   = pipeline.predict_proba(X_first30)[:, 1]\n",
    "\n",
    "# 4. Compile and display\n",
    "results = pd.DataFrame({\n",
    "    \"allow_actual\":        y_first30.values,\n",
    "    \"predicted_class\":     pred_classes,\n",
    "    \"approve_probability\": np.round(pred_probs, 3)\n",
    "}, index=X_first30.index)\n",
    "\n",
    "print(\"First 30 rows: actual vs predicted\\n\")\n",
    "print(results.to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e9daa351-2fb2-4568-86e5-bd564280c320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 767 rows → scored_full_dataset.xlsx\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Load your model and full dataset\n",
    "pipeline = joblib.load(\"xgb_credit_risk_pipeline_final.pkl\")\n",
    "df_full  = pd.read_excel(\"Cargo_final_timestamp_speed_encoded.xlsx\")\n",
    "\n",
    "# 2. Score all rows at once\n",
    "features = df_full.drop(\"allow\", axis=1)\n",
    "df_full[\"predicted_class\"]     = pipeline.predict(features)\n",
    "df_full[\"approve_probability\"] = pipeline.predict_proba(features)[:, 1]\n",
    "\n",
    "# 3. Save the scored file\n",
    "df_full.to_excel(\"scored_full_dataset.xlsx\", index=False)\n",
    "print(f\"Scored {len(df_full)} rows → scored_full_dataset.xlsx\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef51de4-7906-4c84-b1ed-563fc3776dbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
